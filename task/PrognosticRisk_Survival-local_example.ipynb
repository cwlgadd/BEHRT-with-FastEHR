{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEHRT for survival\n",
    "\n",
    "``` This is a modified version of the NextVIsit-6month.ipynb script```\n",
    "\n",
    "This notebook has been changed to consider a survival downstream task. Where possible I have kept the original code structure.\n",
    "\n",
    "We benchmark different datasets, considering two experimental setups for each:\n",
    "- **Scratch**.\n",
    "      This is the Scratch Fine-tuning (SFT) set-up presented in SurvivEHR\n",
    "- **Pre-trained**.\n",
    "      This is different to Full Fine-tuning (FFT) presentation in SurvivEHR.\n",
    "      Here we pre-train on the MLM pre-training objective, but only the samples in the fine-tuning dataset given.\n",
    "      This is because the BEHRT code does not scale (either in terms of compute, nor memory) to the number of pre-training samples used in SurvivEHR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"local_example\"\n",
    "from_pretrained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook results:\n",
    "\n",
    "### Local example\n",
    "\n",
    "**Scratch**\n",
    "\n",
    "```\n",
    "Test results after: 105 epochs\t\n",
    "| Loss: -0.005824794474989176\t\n",
    "| Ctd: 0.865399389194198\t\n",
    "| IBS: 0.04994151870417752\t\n",
    "| INBLL: 0.17613763390130782\t \n",
    "```\n",
    "\n",
    "**Semi pre-trained**\n",
    "\n",
    "```\n",
    "Test results after: 116 epochs\t\n",
    "| Loss: -0.006569801524281502\t\n",
    "| Ctd: 0.8712817274729252\t\n",
    "| IBS: 0.04975835580177876\t\n",
    "| INBLL: 0.1758127616397256\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/BEHRT-with-FastEHR/my-virtual-env-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/BEHRT-with-FastEHR/task\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/BEHRT-with-FastEHR/my-virtual-env-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from common.common import create_folder,load_obj\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from model.utils import age_vocab\n",
    "from model import optimiser\n",
    "from dataLoader.Survival import Survival\n",
    "from dataLoader.utils import seq_padding,code2index, position_idx, index_seg\n",
    "from DeSurv.src.classes import ODESurvSingle, ODESurvMultiple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_example\n"
     ]
    }
   ],
   "source": [
    "match dataset:\n",
    "    case \"local_example\":\n",
    "        file_config = {\n",
    "            'vocab':'data/local_example/token2idx',  # vocabulary idx2token, token2idx\n",
    "            'train': 'data/local_example/data_train.parquet',  # formated data \n",
    "            'test': 'data/local_example/data_test.parquet',  # formated data \n",
    "            'val': 'data/local_example/data_val.parquet',  # formated data \n",
    "            'model_path': f'data/{dataset}/', # where to save model\n",
    "            'pretrainModel': 'local_MLM-notebook.ckpt', # pre-trained model name\n",
    "            'model_name': 'local_MLMSurv-notebook.ckpt', # model name\\\n",
    "            'file_name': 'local_MLMSurv-notebook.out',  # log path\n",
    "            'event_code': [\"DEATH\"],\n",
    "            \"competing_risks\": False,\n",
    "            'freeze_backbone': False, # Whether to train the BEHRT encoder architecture (False) or not (True)\n",
    "        }\n",
    "    case \"fastehr_example\":\n",
    "        file_config = {\n",
    "            'vocab':'/rds/homes/g/gaddcz/Projects/FastEHR/examples/data/_built/adapted/BEHRT/T2D_hypertension/token2idx',  # vocabulary idx2token, token2idx\n",
    "            'train': '/rds/homes/g/gaddcz/Projects/FastEHR/examples/data/_built/adapted/BEHRT/T2D_hypertension/dataset.parquet',  # formated data\n",
    "            'test': None,  # formated data \n",
    "            'val': None,  # formated data \n",
    "            'model_path': f'data/{dataset}/', # where to save model\n",
    "            'pretrainModel': 'fastehr_MLM-notebook.ckpt', # pre-trained model name\n",
    "            'model_name': 'fastehr_MLMSurv-notebook.ckpt', # model name\\\n",
    "            'file_name': 'fastehr_MLMSurv-notebook.out',  # log path\n",
    "            'freeze_backbone': False,  # Whether to train the BEHRT encoder architecture (False) or not (True)\n",
    "        }\n",
    "    case \"hypertension\":\n",
    "        file_config = {\n",
    "            'vocab':'/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/BEHRT/token2idx',  # vocabulary idx2token, token2idx\n",
    "            'train': '/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/BEHRT/train_dataset.parquet',  # formated data \n",
    "            'test': '/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/BEHRT/test_dataset.parquet',  # formated data \n",
    "            'val': '/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/BEHRT/val_dataset.parquet',  # formated data \n",
    "            'model_path': f'data/{dataset}/', # where to save model\n",
    "            'pretrainModel': 'hypertension_MLM-notebook.ckpt', # pre-trained model name\n",
    "            'model_name': 'hypertension_MLMSurv-notebook.ckpt', # model name\\\n",
    "            'file_name': 'hypertension_MLMSurv-notebook.out',  # log path\n",
    "            \"event_code\": [\"HYPERTENSION\"],\n",
    "            \"competing_risks\": False,\n",
    "            'freeze_backbone': False, # Whether to train the BEHRT encoder architecture (False) or not (True)\n",
    "        }\n",
    "    case \"cvd\":\n",
    "        file_config = {\n",
    "            'vocab':'/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/BEHRT/token2idx',  # vocabulary idx2token, token2idx\n",
    "            'train': '/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/BEHRT/train_dataset.parquet',  # formated data \n",
    "            'test': '/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/BEHRT/test_dataset.parquet',  # formated data \n",
    "            'val': '/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/BEHRT/val_dataset.parquet',  # formated data \n",
    "            'model_path': f'data/{dataset}/', # where to save model\n",
    "            'pretrainModel': 'cvd_MLM-notebook.ckpt', # pre-trained model name\n",
    "            'model_name': 'cvd_MLMSurv-notebook.ckpt', # model name\\\n",
    "            'file_name': 'cvd_MLMSurv-notebook.out',  # log path\n",
    "            \"event_code\": [\"IHDINCLUDINGMI_OPTIMALV2\", \"ISCHAEMICSTROKE_V2\", \"MINFARCTION\", \"STROKEUNSPECIFIED_V2\", \"STROKE_HAEMRGIC\"],\n",
    "            \"competing_risks\": True,\n",
    "            'freeze_backbone': False,  # Whether to train the BEHRT encoder architecture (False) or not (True)\n",
    "        }\n",
    "    \n",
    "    case _:\n",
    "        raise NotImplementedError\n",
    "\n",
    "if from_pretrained is False:\n",
    "    file_config[\"pretrainModel\"] = None\n",
    "    \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 64,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': file_config[\"model_path\"],  # output dir\n",
    "    'best_name': file_config[\"file_name\"], # output model name\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 64,    # Modified as shipped version was a bug. Cannot load previous checkpoint if you modify the architecture\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 1        # As during modified pre-training, we reduce to one (from 5) to be comparable\n",
    "}\n",
    "\n",
    "pretrainModel = file_config[\"pretrainModel\"]  # MLM pretrained model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(global_params['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed for new task\n",
    "\n",
    "# def format_label_vocab(token2idx):\n",
    "#     token2idx = token2idx.copy()\n",
    "#     del token2idx['PAD']\n",
    "#     del token2idx['SEP']\n",
    "#     del token2idx['CLS']\n",
    "#     del token2idx['MASK']\n",
    "#     token = list(token2idx.keys())\n",
    "#     labelVocab = {}\n",
    "#     for i,x in enumerate(token):\n",
    "#         labelVocab[x] = i\n",
    "#     return labelVocab\n",
    "\n",
    "# Vocab_diag = format_label_vocab(BertVocab['token2idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()), # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()), # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.2, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.22, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'age': True,\n",
    "    'seg': True,\n",
    "    'posi': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing class block\n",
    "\n",
    "#\n",
    "#\n",
    "# Removed as this is handled in FastEHR\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, segment, age\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.segment_embeddings = nn.Embedding(config.seg_vocab_size, config.hidden_size)\n",
    "        self.age_embeddings = nn.Embedding(config.age_vocab_size, config.hidden_size)\n",
    "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
    "            from_pretrained(embeddings=self._init_posi_embedding(config.max_position_embeddings, config.hidden_size))\n",
    "\n",
    "        self.LayerNorm = Bert.modeling.BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, word_ids, age_ids=None, seg_ids=None, posi_ids=None, age=True):\n",
    "        if seg_ids is None:\n",
    "            seg_ids = torch.zeros_like(word_ids)\n",
    "        if age_ids is None:\n",
    "            age_ids = torch.zeros_like(word_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(word_ids)\n",
    "\n",
    "        word_embed = self.word_embeddings(word_ids)\n",
    "        segment_embed = self.segment_embeddings(seg_ids)\n",
    "        age_embed = self.age_embeddings(age_ids)\n",
    "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
    "        \n",
    "        embeddings = word_embed\n",
    "        \n",
    "        if self.feature_dict['age']:\n",
    "            embeddings = embeddings + age_embed\n",
    "        if self.feature_dict['seg']:\n",
    "            embeddings = embeddings + segment_embed\n",
    "        if self.feature_dict['posi']:\n",
    "            embeddings = embeddings + posi_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
    "        def even_code(pos, idx):\n",
    "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        def odd_code(pos, idx):\n",
    "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
    "\n",
    "        # initialize position embedding table\n",
    "        lookup_table = np.zeros((max_position_embedding, hidden_size), dtype=np.float32)\n",
    "\n",
    "        # reset table parameters with hard encoding\n",
    "        # set even dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(0, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = even_code(pos, idx)\n",
    "        # set odd dimension\n",
    "        for pos in range(max_position_embedding):\n",
    "            for idx in np.arange(1, hidden_size, step=2):\n",
    "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
    "\n",
    "        return torch.tensor(lookup_table)\n",
    "\n",
    "\n",
    "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
    "    def __init__(self, config, feature_dict):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
    "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
    "        self.pooler = Bert.modeling.BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if age_ids is None:\n",
    "            age_ids = torch.zeros_like(input_ids)\n",
    "        if seg_ids is None:\n",
    "            seg_ids = torch.zeros_like(input_ids)\n",
    "        if posi_ids is None:\n",
    "            posi_ids = torch.zeros_like(input_ids)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, age_ids, seg_ids, posi_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "# Previous task wrapper\n",
    "\n",
    "# class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
    "#     def __init__(self, config, num_labels, feature_dict):\n",
    "#         super(BertForMultiLabelPrediction, self).__init__(config)\n",
    "#         self.num_labels = num_labels\n",
    "#         self.bert = BertModel(config, feature_dict)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "#         self.apply(self.init_bert_weights)\n",
    "\n",
    "#     def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
    "#         _, pooled_output = self.bert(input_ids, age_ids ,seg_ids, posi_ids, attention_mask,\n",
    "#                                      output_all_encoded_layers=False)\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.MultiLabelSoftMarginLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "#             return loss, logits\n",
    "#         else:\n",
    "#             return logits\n",
    "\n",
    "# New task wrapper\n",
    "\n",
    "class BertForSurvival(Bert.modeling.BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 config, \n",
    "                 feature_dict,\n",
    "                 competing_risks=False,\n",
    "                 num_risks=None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(BertForSurvival, self).__init__(config)\n",
    "        self.bert = BertModel(config, feature_dict)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.competing_risks = competing_risks\n",
    "        self.num_risks = num_risks\n",
    "        self.t_eval = np.linspace(0, 1, 1000)\n",
    "    \n",
    "        if competing_risks:\n",
    "            assert num_risks is not None\n",
    "            print(f\"Using ODESurvMultiple with causes: {num_risks}\")\n",
    "            self.desurv_model = ODESurvMultiple(\n",
    "                lr=optim_config['lr'],\n",
    "                cov_dim=config.hidden_size,\n",
    "                hidden_dim=32,\n",
    "                num_risks=num_risks,\n",
    "                device=\"cpu\" if global_params[\"device\"] == \"cpu\" else \"gpu\",\n",
    "            )\n",
    "            self.device = self.desurv_model.odenet.device\n",
    "        \n",
    "        else:\n",
    "            print(f\"Using ODESurvSingle for union of given events\")\n",
    "            self.desurv_model = ODESurvSingle(\n",
    "                lr=optim_config['lr'],\n",
    "                cov_dim=config.hidden_size,\n",
    "                hidden_dim=32,\n",
    "                device=\"cpu\" if global_params[\"device\"] == \"cpu\" else \"gpu\",\n",
    "            )\n",
    "            self.device = self.desurv_model.net.device\n",
    "            \n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                target_label, target_time, patient_id,\n",
    "                input_ids,\n",
    "                age_ids=None,\n",
    "                seg_ids=None,\n",
    "                posi_ids=None,\n",
    "                attention_mask=None,\n",
    "                # labels=None\n",
    "               ):\n",
    "        \n",
    "        _, pooled_output = self.bert(input_ids, age_ids ,seg_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Forward DeSurv model for survival prediction using pooled_output as input features\n",
    "        argsort_t = torch.argsort(target_time)\n",
    "        x_ = pooled_output[argsort_t,:].to(global_params[\"device\"])\n",
    "        t_ = target_time[argsort_t].to(global_params[\"device\"])\n",
    "        k_ = target_label[argsort_t].to(global_params[\"device\"])\n",
    "        \n",
    "        return self.desurv_model.forward(x_,t_,k_)\n",
    "\n",
    "    def predict(self, *args, **kwargs):\n",
    "        if self.competing_risks:\n",
    "            return self.predict_cr(*args, **kwargs)\n",
    "        else:\n",
    "            return self.predict_sr(*args, **kwargs)\n",
    "            \n",
    "    def predict_cr(self,\n",
    "                   input_ids,\n",
    "                   age_ids=None,\n",
    "                   seg_ids=None,\n",
    "                   posi_ids=None,\n",
    "                   attention_mask=None,                \n",
    "                  ):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids, age_ids ,seg_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # The normalised grid over which to predict\n",
    "        t_test = torch.tensor(np.concatenate([self.t_eval] * pooled_output.shape[0], 0), dtype=torch.float32, device=self.device) \n",
    "        H_test = pooled_output.repeat_interleave(self.t_eval.size, 0).to(self.device, torch.float32)\n",
    "\n",
    "        # Batched predict: Cannot make all predictions at once due to memory constraints\n",
    "        pred_bsz = 512                                                        # Predict in batches\n",
    "        pred = []\n",
    "        pi = []\n",
    "        for H_test_batched, t_test_batched in zip(torch.split(H_test, pred_bsz), torch.split(t_test, pred_bsz)):\n",
    "            _pred, _pi = self.desurv_model.predict(H_test_batched, t_test_batched)\n",
    "            pred.append(_pred)\n",
    "            pi.append(_pi)\n",
    "\n",
    "        pred = torch.concat(pred)\n",
    "        pi = torch.concat(pi)\n",
    "        pred = pred.reshape((pooled_output.shape[0], self.t_eval.size, -1)).cpu().detach().numpy()\n",
    "        pi = pi.reshape((pooled_output.shape[0], self.t_eval.size, -1)).cpu().detach().numpy()\n",
    "        preds = [pred[:, :, _i] for _i in range(pred.shape[-1])]\n",
    "        pis = [pi[:, :, _i] for _i in range(pi.shape[-1])]\n",
    "\n",
    "        return preds, pis\n",
    "\n",
    "    def predict_sr(self,\n",
    "                   input_ids,\n",
    "                   age_ids=None,\n",
    "                   seg_ids=None,\n",
    "                   posi_ids=None,\n",
    "                   attention_mask=None,  \n",
    "                  ):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids, age_ids ,seg_ids, posi_ids, attention_mask,\n",
    "                                     output_all_encoded_layers=False)\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # The normalised grid over which to predict\n",
    "        t_test = torch.tensor(np.concatenate([self.t_eval] * pooled_output.shape[0], 0), dtype=torch.float32, device=self.device)\n",
    "        H_test = pooled_output.repeat_interleave(self.t_eval.size, 0).to(self.device, torch.float32)\n",
    "\n",
    "        # Batched predict: Cannot make all predictions at once due to memory constraints\n",
    "        pred_bsz = 512                                                        # Predict in batches\n",
    "        pred = []\n",
    "        pi = []\n",
    "        for H_test_batched, t_test_batched in zip(torch.split(H_test, pred_bsz), torch.split(t_test, pred_bsz)):\n",
    "            _pred = self.desurv_model.predict(H_test_batched, t_test_batched)\n",
    "            pred.append(_pred)\n",
    "        pred = [torch.concat(pred).reshape(pooled_output.shape[0], self.t_eval.size).cpu().detach().numpy()]\n",
    "\n",
    "        return pred, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a single-risk model with union of events: ['DEATH']\n",
      "These are combined to outcomes with label2idx mapping:\n",
      "0: 11 keys are considered right-censored events\n",
      "1: ['DEATH']\n"
     ]
    }
   ],
   "source": [
    "# Added code that is used for the survival target labels\n",
    "if file_config[\"competing_risks\"]:\n",
    "    # for competing risks, convert event codes to one-hot (1,2,3,..., K if in event_code, else 0)\n",
    "    event_pos = {code: i + 1 for i, code in enumerate(file_config[\"event_code\"])}\n",
    "    label2idx = {k: event_pos.get(k, 0) for k in BertVocab['token2idx']}\n",
    "    print(f\"Using a competing risks model with causes: {file_config['event_code']}\")\n",
    "else:\n",
    "    # for single-risk, convert event codes to binary (1 if in event_code, else 0)\n",
    "    label2idx = {key:1 if key in file_config[\"event_code\"] else 0 for key, item in BertVocab['token2idx'].items()}\n",
    "    print(f\"Using a single-risk model with union of events: {file_config['event_code']}\")\n",
    "\n",
    "# Report for sanity checking\n",
    "from collections import defaultdict\n",
    "def group_keys_by_value(d, *, sort_keys=False):\n",
    "    groups = defaultdict(list)\n",
    "    for k, v in d.items():\n",
    "        groups[v].append(k)\n",
    "    if sort_keys:\n",
    "        for v in groups:\n",
    "            groups[v].sort()\n",
    "    return dict(groups)\n",
    "    \n",
    "groups = group_keys_by_value(label2idx, sort_keys=True)\n",
    "\n",
    "print(f\"These are combined to outcomes with label2idx mapping:\")\n",
    "for idx, (v, keys) in enumerate(groups.items()):\n",
    "    if idx == 0:\n",
    "        print(f\"{v}: {len(keys)} keys are considered right-censored events\")\n",
    "    else:\n",
    "        print(f\"{v}: {keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "# data['label'] = data.label.apply(lambda x: list(set(x)))\n",
    "# Dset = NextVisit(token2idx=BertVocab['token2idx'], diag2idx=Vocab_diag, age2idx=ageVocab,dataframe=data, max_len=global_params['max_len_seq'])\n",
    "# trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=3)\n",
    "\n",
    "# from dataLoader.MLM import MLMLoader\n",
    "# Dset = MLMLoader(data, BertVocab['token2idx'], ageVocab, max_len=global_params['max_len_seq'], code='caliber_id')\n",
    "# trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "data = pd.read_parquet(file_config['train'])\n",
    "# remove patients with visits less than min visit\n",
    "data['length'] = data['caliber_id'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
    "data = data[data['length'] >= global_params['min_visit']]\n",
    "data = data.reset_index(drop=True)\n",
    "data[\"patid\"] = data.index\n",
    "\n",
    "\n",
    "Dset = Survival(\n",
    "    BertVocab['token2idx'],\n",
    "    label2idx,\n",
    "    ageVocab,\n",
    "    data,\n",
    "    max_len=global_params['max_len_seq'],\n",
    "    code='caliber_id',\n",
    "    label=\"target_event\",\n",
    "    label_time=\"target_time\",\n",
    "    )\n",
    "trainload = DataLoader(\n",
    "    dataset=Dset,\n",
    "    batch_size=global_params['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(file_config['test']).reset_index(drop=True)\n",
    "# data['label'] = data.label.apply(lambda x: list(set(x)))\n",
    "# Dset = NextVisit(token2idx=BertVocab['token2idx'], diag2idx=Vocab_diag, age2idx=ageVocab,dataframe=data, max_len=global_params['max_len_seq'])\n",
    "# testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "if file_config[\"test\"] is not None:\n",
    "    data = pd.read_parquet(file_config['test'])\n",
    "    # remove patients with visits less than min visit\n",
    "    data['length'] = data['caliber_id'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
    "    data = data[data['length'] >= global_params['min_visit']]\n",
    "    data = data.reset_index(drop=True)\n",
    "    data[\"patid\"] = data.index\n",
    "    \n",
    "    \n",
    "    Dset = Survival(\n",
    "        BertVocab['token2idx'],\n",
    "        label2idx,\n",
    "        ageVocab,\n",
    "        data,\n",
    "        max_len=global_params['max_len_seq'],\n",
    "        code='caliber_id',\n",
    "        label=\"target_event\",\n",
    "        label_time=\"target_time\",\n",
    "        )\n",
    "    testload = DataLoader(\n",
    "        dataset=Dset,\n",
    "        batch_size=global_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=3\n",
    "        )\n",
    "\n",
    "# Added validation testing data\n",
    "if file_config[\"val\"] is not None:\n",
    "    data = pd.read_parquet(file_config['val'])\n",
    "    # remove patients with visits less than min visit\n",
    "    data['length'] = data['caliber_id'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
    "    data = data[data['length'] >= global_params['min_visit']]\n",
    "    data = data.reset_index(drop=True)\n",
    "    data[\"patid\"] = data.index\n",
    "    \n",
    "    \n",
    "    Dset = Survival(\n",
    "        BertVocab['token2idx'],\n",
    "        label2idx,\n",
    "        ageVocab,\n",
    "        data,\n",
    "        max_len=global_params['max_len_seq'],\n",
    "        code='caliber_id',\n",
    "        label=\"target_event\",\n",
    "        label_time=\"target_time\",\n",
    "        )\n",
    "    valload = DataLoader(\n",
    "        dataset=Dset,\n",
    "        batch_size=global_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ODESurvSingle for union of given events\n",
      "CondODENet: gpu specified, cuda:0 used\n"
     ]
    }
   ],
   "source": [
    "conf = BertConfig(model_config)\n",
    "model = BertForSurvival(conf, \n",
    "                        feature_dict,\n",
    "                        competing_risks=file_config[\"competing_risks\"],\n",
    "                        num_risks=len(file_config[\"event_code\"])\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_MLM-notebook.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(pretrainModel)\n",
    "\n",
    "if pretrainModel is not None:\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(file_config[\"model_path\"] + pretrainModel)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict) \n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(global_params['device'])\n",
    "# optim = optimiser.adam(params=list(model.named_parameters()), config=optim_config)              # Removed as originally code overwrites this again later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# def precision(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "#     label, output=label.cpu(), output.detach().cpu()\n",
    "#     tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "#     return tempprc, output, label\n",
    "\n",
    "# def precision_test(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "#     tempprc= sklearn.metrics.average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
    "# #     roc = sklearn.metrics.roc_auc_score()\n",
    "#     return tempprc, output, label\n",
    "\n",
    "# def auroc_test(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "#     tempprc= sklearn.metrics.roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
    "# #     roc = sklearn.metrics.roc_auc_score()\n",
    "#     return tempprc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-hot Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# mlb = MultiLabelBinarizer(classes=list(Vocab_diag.values()))\n",
    "# mlb.fit([[each] for each in list(Vocab_diag.values())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for step, batch in enumerate(trainload):\n",
    "#     age_ids, input_ids, posi_ids, segment_ids, attMask, target_label, target_time, patient_id = batch\n",
    "#     # age_ids, input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "#     # loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids,attention_mask=attMask, masked_lm_labels=masked_label)\n",
    "\n",
    "#     target_time = target_time.squeeze(-1).to(global_params['device'])         # [64]\n",
    "#     target_label = target_label.squeeze(-1).to(global_params['device'])         # [64]\n",
    "    \n",
    "#     loss = model.desurv_model(torch.ones(target_time.shape[0], 288).to(global_params['device']), target_time, target_label)\n",
    "#     if type(loss) is not np.int:\n",
    "#         print(target_time)\n",
    "#         print(target_label)\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt +=1\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, target_label, target_time, patient_id = batch\n",
    "        \n",
    "        patient_id  = patient_id.squeeze(-1).to(global_params['device'])          # [64]\n",
    "        target_time = target_time.squeeze(-1).to(global_params['device'])         # [64]\n",
    "        target_label = target_label.squeeze(-1).to(global_params['device'])         # [64]\n",
    "\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        \n",
    "        loss = model(target_label, \n",
    "                     target_time, \n",
    "                     patient_id,\n",
    "                     input_ids, \n",
    "                     age_ids, \n",
    "                     segment_ids,\n",
    "                     posi_ids,\n",
    "                     attention_mask=attMask\n",
    "                     )\n",
    "        \n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss/global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if step % 2000==0:\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t\".format(e, cnt, temp_loss/2000))\n",
    "            temp_loss = 0\n",
    "        \n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "def validation(loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_samples = 0\n",
    "    for step, batch in enumerate(loader):\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, target_label, target_time, patient_id = batch\n",
    "        \n",
    "        patient_id  = patient_id.squeeze(-1).to(global_params['device'])          # [64]\n",
    "        target_time = target_time.squeeze(-1).to(global_params['device'])         # [64]\n",
    "        target_label = target_label.squeeze(-1).to(global_params['device'])         # [64]\n",
    "\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        \n",
    "        loss = model(target_label, \n",
    "                     target_time, \n",
    "                     patient_id,\n",
    "                     input_ids, \n",
    "                     age_ids, \n",
    "                     segment_ids,\n",
    "                     posi_ids,\n",
    "                     attention_mask=attMask\n",
    "                     )\n",
    "\n",
    "        val_samples += target_label.shape[0]\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= val_samples\n",
    "    return val_loss\n",
    "\n",
    "def evaluation(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ctd, ibs, inbll = [], [], []\n",
    "    total_samples = 0\n",
    "    for step, batch in enumerate(loader):\n",
    "        \n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, target_label, target_time, patient_id = batch\n",
    "        \n",
    "        patient_id  = patient_id.squeeze(-1).to(global_params['device'])          # [64]\n",
    "        target_time = target_time.squeeze(-1).to(global_params['device'])         # [64]\n",
    "        target_label = target_label.squeeze(-1).to(global_params['device'])         # [64]\n",
    "\n",
    "        age_ids = age_ids.to(global_params['device'])\n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred_surv_CDFs, _ = model.predict(\n",
    "                input_ids, \n",
    "                age_ids, \n",
    "                segment_ids,\n",
    "                posi_ids,\n",
    "                attention_mask=attMask\n",
    "            )\n",
    "\n",
    "            # Convert to numpy\n",
    "            target_time = target_time.cpu().numpy()\n",
    "            target_label = target_label.cpu().numpy()\n",
    "\n",
    "            # Calculate the metrics by combining outcomes\n",
    "            cdf = np.zeros_like(pred_surv_CDFs[0])\n",
    "            lbls = np.zeros(target_label.shape)\n",
    "            for outcome in range(len(pred_surv_CDFs)):\n",
    "                lbls += (target_label == outcome + 1)\n",
    "                cdf += pred_surv_CDFs[outcome]\n",
    "\n",
    "            try:\n",
    "                surv = pd.DataFrame(np.transpose((1 - cdf)), index=model.t_eval)\n",
    "                ev = EvalSurv(surv, target_time, lbls, censor_surv='km')\n",
    "                \n",
    "                time_grid = np.linspace(start=0, stop=model.t_eval.max() , num=300)\n",
    "                batch_ctd = ev.concordance_td() \n",
    "                batch_ibs = ev.integrated_brier_score(time_grid) \n",
    "                batch_inbll = ev.integrated_nbll(time_grid)\n",
    "                \n",
    "                # log \n",
    "                # Note: this is will handle batches with no comparable pairs the same way as SurvivEHR's callback\n",
    "                ctd.append(batch_ctd)\n",
    "                ibs.append(batch_ibs)\n",
    "                inbll.append(batch_inbll)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    ctd = np.mean(ctd)\n",
    "    ibs = np.mean(ibs)\n",
    "    inbll = np.mean(inbll)\n",
    "\n",
    "    return ctd, ibs, inbll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: modification\n",
    "\n",
    "In the original code the test loader is used to perform early stopping. This is very bad and leads to over reporting performance. \n",
    "\n",
    "Ideally this would be done with a validation split, but in-keeping with original code I have swapped it to the training split. \n",
    "I also added a max_wait on the early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "optim_config = {\n",
    "    'lr': 3e-6,\n",
    "    'warmup_proportion': 0.1\n",
    "}\n",
    "\n",
    "\n",
    "if file_config[\"freeze_backbone\"]:\n",
    "    optim = optimiser.adam(params=list(model.desurv_model.named_parameters()), config=optim_config)\n",
    "else:\n",
    "    optim = optimiser.adam(params=list(model.named_parameters()), config=optim_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t| Cnt: 1\t| Loss: 0.015326277732849121\t\n",
      "epoch: 0\t| Loss: 0.4689064998626709\t| Ctd: 0.7903810477959013\t| IBS: 0.15867840170439443\t| INBLL: 0.4747107610965897\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t| Cnt: 1\t| Loss: 0.01464528465270996\t\n",
      "epoch: 1\t| Loss: 0.4598592948913574\t| Ctd: 0.7295976506621408\t| IBS: 0.15589206695261631\t| INBLL: 0.4691069345651151\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t| Cnt: 1\t| Loss: 0.013576499938964844\t\n",
      "epoch: 2\t| Loss: 0.45254419898986814\t| Ctd: 0.7217130226228331\t| IBS: 0.15360373830457946\t| INBLL: 0.46460353867567716\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t| Cnt: 1\t| Loss: 0.014366779327392578\t\n",
      "epoch: 3\t| Loss: 0.44506317329406736\t| Ctd: 0.7218224904744226\t| IBS: 0.15127103574218126\t| INBLL: 0.4600020822918534\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 4\t| Cnt: 1\t| Loss: 0.013560546875\t\n",
      "epoch: 4\t| Loss: 0.4373501968383789\t| Ctd: 0.7261859774902348\t| IBS: 0.14887524016590714\t| INBLL: 0.4552697241765564\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 5\t| Cnt: 1\t| Loss: 0.013209809303283691\t\n",
      "epoch: 5\t| Loss: 0.4292624969482422\t| Ctd: 0.7242553625804993\t| IBS: 0.1463736935094928\t| INBLL: 0.4503188729802995\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 6\t| Cnt: 1\t| Loss: 0.012738675117492675\t\n",
      "epoch: 6\t| Loss: 0.4206554908752441\t| Ctd: 0.7268816893630153\t| IBS: 0.14372540903692696\t| INBLL: 0.44506267462867893\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 7\t| Cnt: 1\t| Loss: 0.012627223968505859\t\n",
      "epoch: 7\t| Loss: 0.411724328994751\t| Ctd: 0.7265290371088885\t| IBS: 0.1409934954585162\t| INBLL: 0.4396281751287926\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 8\t| Cnt: 1\t| Loss: 0.012646711349487305\t\n",
      "epoch: 8\t| Loss: 0.4025240249633789\t| Ctd: 0.7280747126255617\t| IBS: 0.13819886499989315\t| INBLL: 0.4340496256581354\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 9\t| Cnt: 1\t| Loss: 0.0145543794631958\t\n",
      "epoch: 9\t| Loss: 0.39306265068054197\t| Ctd: 0.731547362412067\t| IBS: 0.13534865122040582\t| INBLL: 0.42833054077126026\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 10\t| Cnt: 1\t| Loss: 0.013216928482055664\t\n",
      "epoch: 10\t| Loss: 0.38339494228363036\t| Ctd: 0.7329504526703898\t| IBS: 0.13246395971674801\t| INBLL: 0.4225089812250641\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 11\t| Cnt: 1\t| Loss: 0.011342962265014649\t\n",
      "epoch: 11\t| Loss: 0.3734447774887085\t| Ctd: 0.7322130609809313\t| IBS: 0.12952709726506256\t| INBLL: 0.4165324877687767\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 12\t| Cnt: 1\t| Loss: 0.010492748260498046\t\n",
      "epoch: 12\t| Loss: 0.3632069721221924\t| Ctd: 0.7434262165407565\t| IBS: 0.12654198402640698\t| INBLL: 0.4103994552188359\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 13\t| Cnt: 1\t| Loss: 0.011328302383422851\t\n",
      "epoch: 13\t| Loss: 0.352765606880188\t| Ctd: 0.7352748694086244\t| IBS: 0.12353870884422509\t| INBLL: 0.4041605844437252\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 14\t| Cnt: 1\t| Loss: 0.0101354341506958\t\n",
      "epoch: 14\t| Loss: 0.3421448335647583\t| Ctd: 0.7395506805612471\t| IBS: 0.1205304707963057\t| INBLL: 0.39783413716282384\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 15\t| Cnt: 1\t| Loss: 0.009836956024169922\t\n",
      "epoch: 15\t| Loss: 0.33134692001342775\t| Ctd: 0.7438091456990561\t| IBS: 0.11752285289514304\t| INBLL: 0.391416637475109\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 16\t| Cnt: 1\t| Loss: 0.00927433681488037\t\n",
      "epoch: 16\t| Loss: 0.3204801521301269\t| Ctd: 0.7402262560493174\t| IBS: 0.11455062364088901\t| INBLL: 0.38497016825045804\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 17\t| Cnt: 1\t| Loss: 0.009018457412719727\t\n",
      "epoch: 17\t| Loss: 0.3095767726898193\t| Ctd: 0.7524173465318356\t| IBS: 0.11162801761612459\t| INBLL: 0.3785154351740903\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 18\t| Cnt: 1\t| Loss: 0.009118675231933594\t\n",
      "epoch: 18\t| Loss: 0.2986783504486084\t| Ctd: 0.7479615228289325\t| IBS: 0.10876809461629296\t| INBLL: 0.37206973301603563\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 19\t| Cnt: 1\t| Loss: 0.009754341125488281\t\n",
      "epoch: 19\t| Loss: 0.2879116363525391\t| Ctd: 0.7573053415348012\t| IBS: 0.10600722344190379\t| INBLL: 0.3657070512298503\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 20\t| Cnt: 1\t| Loss: 0.00919318962097168\t\n",
      "epoch: 20\t| Loss: 0.2773339014053345\t| Ctd: 0.758135531640814\t| IBS: 0.10335865437525504\t| INBLL: 0.35945377983230153\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 21\t| Cnt: 1\t| Loss: 0.007377157211303711\t\n",
      "epoch: 21\t| Loss: 0.26692928886413575\t| Ctd: 0.748416013455718\t| IBS: 0.10081520306977065\t| INBLL: 0.3532920061251803\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 22\t| Cnt: 1\t| Loss: 0.008954399108886719\t\n",
      "epoch: 22\t| Loss: 0.25693044185638425\t| Ctd: 0.7465934743824016\t| IBS: 0.0984296490071743\t| INBLL: 0.3473539241766287\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 23\t| Cnt: 1\t| Loss: 0.007896098136901855\t\n",
      "epoch: 23\t| Loss: 0.24728502559661866\t| Ctd: 0.7562485153195675\t| IBS: 0.09617720693269777\t| INBLL: 0.34159607662970376\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 24\t| Cnt: 1\t| Loss: 0.008383913040161133\t\n",
      "epoch: 24\t| Loss: 0.23806853008270262\t| Ctd: 0.7594027434854831\t| IBS: 0.09408132024573748\t| INBLL: 0.3360729920333689\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 25\t| Cnt: 1\t| Loss: 0.006709193229675293\t\n",
      "epoch: 25\t| Loss: 0.22936310386657716\t| Ctd: 0.7463510290785598\t| IBS: 0.09212455763853732\t| INBLL: 0.330802642111072\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 26\t| Cnt: 1\t| Loss: 0.007060585498809814\t\n",
      "epoch: 26\t| Loss: 0.22114370155334473\t| Ctd: 0.7617198681303661\t| IBS: 0.0902998483597525\t| INBLL: 0.32577944672410825\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 27\t| Cnt: 1\t| Loss: 0.006666565418243408\t\n",
      "epoch: 27\t| Loss: 0.21342902755737306\t| Ctd: 0.761105156207228\t| IBS: 0.08860055567954425\t| INBLL: 0.32101587396560916\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 28\t| Cnt: 1\t| Loss: 0.006323415279388428\t\n",
      "epoch: 28\t| Loss: 0.2061742286682129\t| Ctd: 0.7589653100328074\t| IBS: 0.08699404941649674\t| INBLL: 0.31647979014369004\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 29\t| Cnt: 1\t| Loss: 0.005964995384216309\t\n",
      "epoch: 29\t| Loss: 0.1993486385345459\t| Ctd: 0.7592356651587642\t| IBS: 0.08545808522444351\t| INBLL: 0.312153889747242\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 30\t| Cnt: 1\t| Loss: 0.0045271067619323735\t\n",
      "epoch: 30\t| Loss: 0.1928967351913452\t| Ctd: 0.7912377400939802\t| IBS: 0.08400832733806554\t| INBLL: 0.3080318046697085\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 31\t| Cnt: 1\t| Loss: 0.005072328567504883\t\n",
      "epoch: 31\t| Loss: 0.18674273586273193\t| Ctd: 0.809956051920762\t| IBS: 0.08259280774411162\t| INBLL: 0.30405622026323953\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 32\t| Cnt: 1\t| Loss: 0.00494456672668457\t\n",
      "epoch: 32\t| Loss: 0.180876633644104\t| Ctd: 0.8300982450398854\t| IBS: 0.08121371740231878\t| INBLL: 0.3002331531311666\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 33\t| Cnt: 1\t| Loss: 0.0052111253738403324\t\n",
      "epoch: 33\t| Loss: 0.17517560625076295\t| Ctd: 0.8519529799712212\t| IBS: 0.07985665958389221\t| INBLL: 0.2964955447344667\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 34\t| Cnt: 1\t| Loss: 0.0044296603202819826\t\n",
      "epoch: 34\t| Loss: 0.16963742399215698\t| Ctd: 0.861084945702232\t| IBS: 0.07854015055633681\t| INBLL: 0.29285224970068036\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 35\t| Cnt: 1\t| Loss: 0.006629964828491211\t\n",
      "epoch: 35\t| Loss: 0.1641772994995117\t| Ctd: 0.8605972337641884\t| IBS: 0.07724875922740275\t| INBLL: 0.28925078215228384\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 36\t| Cnt: 1\t| Loss: 0.004951803207397461\t\n",
      "epoch: 36\t| Loss: 0.1588446879386902\t| Ctd: 0.8648804275391548\t| IBS: 0.07596786968532415\t| INBLL: 0.28571874787159895\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 37\t| Cnt: 1\t| Loss: 0.005747217178344727\t\n",
      "epoch: 37\t| Loss: 0.15357761526107788\t| Ctd: 0.8639855966424017\t| IBS: 0.07472018402728883\t| INBLL: 0.28222656056672574\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 38\t| Cnt: 1\t| Loss: 0.004597968578338623\t\n",
      "epoch: 38\t| Loss: 0.1484335083961487\t| Ctd: 0.8661932504142089\t| IBS: 0.07349976552272888\t| INBLL: 0.2788086294655864\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 39\t| Cnt: 1\t| Loss: 0.006057011127471924\t\n",
      "epoch: 39\t| Loss: 0.14329154396057128\t| Ctd: 0.8670237664014091\t| IBS: 0.07229469911144717\t| INBLL: 0.27538986340098237\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 40\t| Cnt: 1\t| Loss: 0.005207715511322021\t\n",
      "epoch: 40\t| Loss: 0.13826925945281981\t| Ctd: 0.8684222875900542\t| IBS: 0.07110487719645622\t| INBLL: 0.27204543718013563\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 41\t| Cnt: 1\t| Loss: 0.0039088790416717525\t\n",
      "epoch: 41\t| Loss: 0.13323954248428344\t| Ctd: 0.8660899895053203\t| IBS: 0.06995913045688892\t| INBLL: 0.26869923816689645\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 42\t| Cnt: 1\t| Loss: 0.005361159801483154\t\n",
      "epoch: 42\t| Loss: 0.128196711063385\t| Ctd: 0.86676299450089\t| IBS: 0.06876474061287116\t| INBLL: 0.2653451991786453\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 43\t| Cnt: 1\t| Loss: 0.003693870782852173\t\n",
      "epoch: 43\t| Loss: 0.12267756795883179\t| Ctd: 0.8660911804361662\t| IBS: 0.06714933989516639\t| INBLL: 0.26167159294757814\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 44\t| Cnt: 1\t| Loss: 0.005805181503295898\t\n",
      "epoch: 44\t| Loss: 0.11683321571350097\t| Ctd: 0.8655566736677586\t| IBS: 0.06580739752272229\t| INBLL: 0.25784188763509586\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 45\t| Cnt: 1\t| Loss: 0.004059562683105469\t\n",
      "epoch: 45\t| Loss: 0.11108009767532348\t| Ctd: 0.8655687676364147\t| IBS: 0.06431903998222668\t| INBLL: 0.2540276385455444\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 46\t| Cnt: 1\t| Loss: 0.003743666172027588\t\n",
      "epoch: 46\t| Loss: 0.1056588168144226\t| Ctd: 0.865311479829194\t| IBS: 0.06282956756380423\t| INBLL: 0.2503705060950844\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 47\t| Cnt: 1\t| Loss: 0.003875879764556885\t\n",
      "epoch: 47\t| Loss: 0.09999492120742798\t| Ctd: 0.8654660776564203\t| IBS: 0.06207176870932947\t| INBLL: 0.24679147195185627\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 48\t| Cnt: 1\t| Loss: 0.00549079704284668\t\n",
      "epoch: 48\t| Loss: 0.09481830501556396\t| Ctd: 0.8650390272329239\t| IBS: 0.06067332728620564\t| INBLL: 0.24327024701176925\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 49\t| Cnt: 1\t| Loss: 0.0038754382133483886\t\n",
      "epoch: 49\t| Loss: 0.08949820852279664\t| Ctd: 0.8654732520821511\t| IBS: 0.05973882999023443\t| INBLL: 0.23981801000562913\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 50\t| Cnt: 1\t| Loss: 0.005059060573577881\t\n",
      "epoch: 50\t| Loss: 0.08458370065689087\t| Ctd: 0.8660786913587486\t| IBS: 0.05870007201991283\t| INBLL: 0.23653457744296003\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 51\t| Cnt: 1\t| Loss: 0.004154957771301269\t\n",
      "epoch: 51\t| Loss: 0.07942037415504456\t| Ctd: 0.8658580863693366\t| IBS: 0.05808960051109402\t| INBLL: 0.23325741098896316\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 52\t| Cnt: 1\t| Loss: 0.003796938896179199\t\n",
      "epoch: 52\t| Loss: 0.07482083201408386\t| Ctd: 0.8651560881045572\t| IBS: 0.057178832186116274\t| INBLL: 0.23017554288093864\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 53\t| Cnt: 1\t| Loss: 0.0034965343475341797\t\n",
      "epoch: 53\t| Loss: 0.07021638226509094\t| Ctd: 0.865496867043491\t| IBS: 0.05650144350833498\t| INBLL: 0.227167784043964\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 54\t| Cnt: 1\t| Loss: 0.00338260293006897\t\n",
      "epoch: 54\t| Loss: 0.06594776558876038\t| Ctd: 0.8653743494420834\t| IBS: 0.055728169859631915\t| INBLL: 0.2242976563791967\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 55\t| Cnt: 1\t| Loss: 0.0026456992626190186\t\n",
      "epoch: 55\t| Loss: 0.06176047730445862\t| Ctd: 0.8656722438640414\t| IBS: 0.055114096159226876\t| INBLL: 0.22153043822629606\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 56\t| Cnt: 1\t| Loss: 0.003669468641281128\t\n",
      "epoch: 56\t| Loss: 0.057727219581604004\t| Ctd: 0.865936717470819\t| IBS: 0.05454822339664863\t| INBLL: 0.21885893616646662\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 57\t| Cnt: 1\t| Loss: 0.0037614459991455076\t\n",
      "epoch: 57\t| Loss: 0.05390558612346649\t| Ctd: 0.8645815955824125\t| IBS: 0.05402112328874914\t| INBLL: 0.21631398666144122\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 58\t| Cnt: 1\t| Loss: 0.0015714550018310547\t\n",
      "epoch: 58\t| Loss: 0.050227078080177304\t| Ctd: 0.8660760818735305\t| IBS: 0.05351260434333577\t| INBLL: 0.21384917291386552\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 59\t| Cnt: 1\t| Loss: 0.0011004695892333984\t\n",
      "epoch: 59\t| Loss: 0.04678564894199371\t| Ctd: 0.8645977626046277\t| IBS: 0.053059155607610556\t| INBLL: 0.21153642508524934\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 60\t| Cnt: 1\t| Loss: 0.004185932159423828\t\n",
      "epoch: 60\t| Loss: 0.043419171929359436\t| Ctd: 0.8646358437703582\t| IBS: 0.05268636758250317\t| INBLL: 0.20929597062611882\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 61\t| Cnt: 1\t| Loss: 0.0008057515621185303\t\n",
      "epoch: 61\t| Loss: 0.040246621131896976\t| Ctd: 0.8654735598680392\t| IBS: 0.052348965175565795\t| INBLL: 0.20717981309555328\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 62\t| Cnt: 1\t| Loss: 0.0033371005058288575\t\n",
      "epoch: 62\t| Loss: 0.03759548723697662\t| Ctd: 0.8646101878112757\t| IBS: 0.05186649999765455\t| INBLL: 0.20526806375041884\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 63\t| Cnt: 1\t| Loss: 0.00250337553024292\t\n",
      "epoch: 63\t| Loss: 0.034337390422821044\t| Ctd: 0.8662721077060059\t| IBS: 0.05177206786246878\t| INBLL: 0.20322592445132384\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 64\t| Cnt: 1\t| Loss: 0.002984498977661133\t\n",
      "epoch: 64\t| Loss: 0.03165684497356415\t| Ctd: 0.8663148046760856\t| IBS: 0.051510827903718005\t| INBLL: 0.20141608351309673\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 65\t| Cnt: 1\t| Loss: 0.0027095296382904054\t\n",
      "epoch: 65\t| Loss: 0.028990476965904236\t| Ctd: 0.8665683769327308\t| IBS: 0.051343514175523165\t| INBLL: 0.199656812829993\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 66\t| Cnt: 1\t| Loss: 0.0034007248878479006\t\n",
      "epoch: 66\t| Loss: 0.026617499113082886\t| Ctd: 0.8669233512893196\t| IBS: 0.051101188336435954\t| INBLL: 0.19803224107588735\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 67\t| Cnt: 1\t| Loss: 0.0010817229747772216\t\n",
      "epoch: 67\t| Loss: 0.02463330614566803\t| Ctd: 0.8673382825258334\t| IBS: 0.050780776860379566\t| INBLL: 0.1965823857508192\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 68\t| Cnt: 1\t| Loss: 0.0017574619054794311\t\n",
      "epoch: 68\t| Loss: 0.02209332275390625\t| Ctd: 0.8656495501175246\t| IBS: 0.05083053959531098\t| INBLL: 0.19501320890792567\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 69\t| Cnt: 1\t| Loss: 0.0027689714431762694\t\n",
      "epoch: 69\t| Loss: 0.020331555008888245\t| Ctd: 0.8662248801072893\t| IBS: 0.0505386036973149\t| INBLL: 0.19371592667092916\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 70\t| Cnt: 1\t| Loss: 0.0007181048393249511\t\n",
      "epoch: 70\t| Loss: 0.018468981266021728\t| Ctd: 0.8686637671021764\t| IBS: 0.05039834564071291\t| INBLL: 0.1924397232738177\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 71\t| Cnt: 1\t| Loss: 0.0018384350538253784\t\n",
      "epoch: 71\t| Loss: 0.016371439456939696\t| Ctd: 0.8684332539452169\t| IBS: 0.05047137755908923\t| INBLL: 0.19114449036226383\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 72\t| Cnt: 1\t| Loss: 0.002201235294342041\t\n",
      "epoch: 72\t| Loss: 0.01498820984363556\t| Ctd: 0.8684007810736833\t| IBS: 0.050215188872248066\t| INBLL: 0.19009493704303426\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 73\t| Cnt: 1\t| Loss: 0.0023140213489532472\t\n",
      "epoch: 73\t| Loss: 0.013293461740016938\t| Ctd: 0.8688074926185139\t| IBS: 0.05020878003182622\t| INBLL: 0.1890028109298802\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 74\t| Cnt: 1\t| Loss: 0.0008340659141540527\t\n",
      "epoch: 74\t| Loss: 0.012032744646072388\t| Ctd: 0.8685124896915496\t| IBS: 0.05004108519173388\t| INBLL: 0.18807145618998464\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 75\t| Cnt: 1\t| Loss: 0.00042072921991348266\t\n",
      "epoch: 75\t| Loss: 0.010912724375724792\t| Ctd: 0.8678033598703085\t| IBS: 0.049898313402131494\t| INBLL: 0.18722963108065482\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 76\t| Cnt: 1\t| Loss: 0.0010109496116638183\t\n",
      "epoch: 76\t| Loss: 0.0095337975025177\t| Ctd: 0.8682276897105206\t| IBS: 0.04988293207600594\t| INBLL: 0.18633093342022583\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 77\t| Cnt: 1\t| Loss: 0.0004023754000663757\t\n",
      "epoch: 77\t| Loss: 0.008382221221923829\t| Ctd: 0.8688701613102595\t| IBS: 0.04982594475083678\t| INBLL: 0.18553666461695775\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 78\t| Cnt: 1\t| Loss: 0.0010119612216949463\t\n",
      "epoch: 78\t| Loss: 0.006939692676067352\t| Ctd: 0.8679965849170737\t| IBS: 0.04991828271990319\t| INBLL: 0.18467970950611934\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 79\t| Cnt: 1\t| Loss: 0.0013765869140625\t\n",
      "epoch: 79\t| Loss: 0.006036919593811035\t| Ctd: 0.8669856172231672\t| IBS: 0.04982436512797757\t| INBLL: 0.1840251311333344\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 80\t| Cnt: 1\t| Loss: 0.0002851454019546509\t\n",
      "epoch: 80\t| Loss: 0.004932065606117249\t| Ctd: 0.8688253106730436\t| IBS: 0.0498503108510522\t| INBLL: 0.1833191663758354\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 81\t| Cnt: 1\t| Loss: 0.0007485219836235046\t\n",
      "epoch: 81\t| Loss: 0.004057677686214447\t| Ctd: 0.8686635870309325\t| IBS: 0.04981417201684116\t| INBLL: 0.18270713937426575\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 82\t| Cnt: 1\t| Loss: 0.0016118273735046386\t\n",
      "epoch: 82\t| Loss: 0.003079181641340256\t| Ctd: 0.8692345293429816\t| IBS: 0.04986728199909537\t| INBLL: 0.18209331308599314\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 83\t| Cnt: 1\t| Loss: -0.0006434974670410156\t\n",
      "epoch: 83\t| Loss: 0.0024726294577121733\t| Ctd: 0.8691161089050643\t| IBS: 0.04976930997327938\t| INBLL: 0.18161594030354772\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 84\t| Cnt: 1\t| Loss: 0.0007891150116920471\t\n",
      "epoch: 84\t| Loss: 0.0014279427528381347\t| Ctd: 0.8686446789556947\t| IBS: 0.04994902476171808\t| INBLL: 0.181042206090557\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 85\t| Cnt: 1\t| Loss: 0.0015122673511505126\t\n",
      "epoch: 85\t| Loss: 0.0009057350754737854\t| Ctd: 0.8680618977297776\t| IBS: 0.049831844076626816\t| INBLL: 0.18066276874308784\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 86\t| Cnt: 1\t| Loss: 0.001413550615310669\t\n",
      "epoch: 86\t| Loss: 0.0003505257964134216\t| Ctd: 0.8679305396008254\t| IBS: 0.04978797233127727\t| INBLL: 0.18025222541510616\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 87\t| Cnt: 1\t| Loss: 0.0005839161872863769\t\n",
      "epoch: 87\t| Loss: -0.00020697882771492004\t| Ctd: 0.8673840041726977\t| IBS: 0.049771018916917605\t| INBLL: 0.17986135019051136\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 88\t| Cnt: 1\t| Loss: 0.0002937391400337219\t\n",
      "epoch: 88\t| Loss: -0.0006809878349304199\t| Ctd: 0.8673667514482604\t| IBS: 0.049727356781822024\t| INBLL: 0.17953486454517462\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 89\t| Cnt: 1\t| Loss: -7.327315211296082e-05\t\n",
      "epoch: 89\t| Loss: -0.0010717068463563918\t| Ctd: 0.8689973416282167\t| IBS: 0.04967342633703917\t| INBLL: 0.17924463761987303\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 90\t| Cnt: 1\t| Loss: 8.910602331161499e-05\t\n",
      "epoch: 90\t| Loss: -0.0018725325167179107\t| Ctd: 0.8687948336550686\t| IBS: 0.04985497667949919\t| INBLL: 0.17883996423142398\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 91\t| Cnt: 1\t| Loss: 0.0017699670791625976\t\n",
      "epoch: 91\t| Loss: -0.0021777988374233246\t| Ctd: 0.8677161439618428\t| IBS: 0.049771044787940935\t| INBLL: 0.17858679774502992\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 92\t| Cnt: 1\t| Loss: 0.00041467368602752684\t\n",
      "epoch: 92\t| Loss: -0.002451229929924011\t| Ctd: 0.8690124996705108\t| IBS: 0.04970449652577774\t| INBLL: 0.17835998087742347\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 93\t| Cnt: 1\t| Loss: -0.0007474126815795899\t\n",
      "epoch: 93\t| Loss: -0.003006548225879669\t| Ctd: 0.8677291965802588\t| IBS: 0.04981408347779524\t| INBLL: 0.17809403326913487\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 94\t| Cnt: 1\t| Loss: 0.000631167709827423\t\n",
      "epoch: 94\t| Loss: -0.0034079822599887848\t| Ctd: 0.8686937574841233\t| IBS: 0.049886016411379346\t| INBLL: 0.17777709893923274\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 95\t| Cnt: 1\t| Loss: 0.0010134886503219605\t\n",
      "epoch: 95\t| Loss: -0.003540971040725708\t| Ctd: 0.8678406059528236\t| IBS: 0.049742966162349384\t| INBLL: 0.177664221118181\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 96\t| Cnt: 1\t| Loss: 0.0007459878325462341\t\n",
      "epoch: 96\t| Loss: -0.003796088144183159\t| Ctd: 0.8689856331567991\t| IBS: 0.049738824067244544\t| INBLL: 0.17745538219520152\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 97\t| Cnt: 1\t| Loss: -0.0007100441455841065\t\n",
      "epoch: 97\t| Loss: -0.004140867829322815\t| Ctd: 0.8681178587391734\t| IBS: 0.04976622607827929\t| INBLL: 0.17729106641219783\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 98\t| Cnt: 1\t| Loss: -0.00013168755173683167\t\n",
      "epoch: 98\t| Loss: -0.004260781705379486\t| Ctd: 0.87004915086015\t| IBS: 0.04968711913192892\t| INBLL: 0.17718077518631803\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 99\t| Cnt: 1\t| Loss: 0.0002442237138748169\t\n",
      "epoch: 99\t| Loss: -0.00447511936724186\t| Ctd: 0.8701444507262297\t| IBS: 0.0496717617785365\t| INBLL: 0.1770595735783323\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 100\t| Cnt: 1\t| Loss: 0.0013866498470306396\t\n",
      "epoch: 100\t| Loss: -0.00456105237454176\t| Ctd: 0.8701556162475059\t| IBS: 0.04961989995157863\t| INBLL: 0.17694569535010848\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 101\t| Cnt: 1\t| Loss: 0.0008003860712051392\t\n",
      "epoch: 101\t| Loss: -0.005079209417104721\t| Ctd: 0.8710190999425301\t| IBS: 0.04978839738391663\t| INBLL: 0.17676507552289578\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 102\t| Cnt: 1\t| Loss: -0.0011631503105163574\t\n",
      "epoch: 102\t| Loss: -0.0052020311802625655\t| Ctd: 0.8716692902741514\t| IBS: 0.04975436427593333\t| INBLL: 0.17659273835900266\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 103\t| Cnt: 1\t| Loss: 0.000260769784450531\t\n",
      "epoch: 103\t| Loss: -0.00537018433958292\t| Ctd: 0.8707862988428456\t| IBS: 0.04976910971750052\t| INBLL: 0.17645748490375052\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 104\t| Cnt: 1\t| Loss: 0.00037144029140472414\t\n",
      "epoch: 104\t| Loss: -0.005518650159239769\t| Ctd: 0.871641227998639\t| IBS: 0.04974942567049104\t| INBLL: 0.1763945095294167\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 105\t| Cnt: 1\t| Loss: -0.0006792794466018677\t\n",
      "epoch: 105\t| Loss: -0.005743930548429489\t| Ctd: 0.8704030954233843\t| IBS: 0.0498187813914979\t| INBLL: 0.1762526456014083\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 106\t| Cnt: 1\t| Loss: -0.000764028549194336\t\n",
      "epoch: 106\t| Loss: -0.005626381449401379\t| Ctd: 0.8693634425191643\t| IBS: 0.04966722368442831\t| INBLL: 0.17625411238484945\t\n",
      "** ** * No improvement in model   ** ** * \n",
      "epoch: 107\t| Cnt: 1\t| Loss: 0.0007067697048187256\t\n",
      "epoch: 107\t| Loss: -0.005552848815917969\t| Ctd: 0.8709495978954693\t| IBS: 0.04956683191656382\t| INBLL: 0.17628035187668534\t\n",
      "** ** * No improvement in model   ** ** * \n",
      "epoch: 108\t| Cnt: 1\t| Loss: -0.0006939777731895446\t\n",
      "epoch: 108\t| Loss: -0.005986767075955868\t| Ctd: 0.8727710091816974\t| IBS: 0.04971067725338809\t| INBLL: 0.17608715869629912\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 109\t| Cnt: 1\t| Loss: 0.0004152495265007019\t\n",
      "epoch: 109\t| Loss: -0.005922628819942474\t| Ctd: 0.8717349940387491\t| IBS: 0.04962908920985584\t| INBLL: 0.1760665303679965\t\n",
      "** ** * No improvement in model   ** ** * \n",
      "epoch: 110\t| Cnt: 1\t| Loss: -0.0005330379605293274\t\n",
      "epoch: 110\t| Loss: -0.0061831907108426095\t| Ctd: 0.8721035384000624\t| IBS: 0.04970760422821054\t| INBLL: 0.1759554805602057\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 111\t| Cnt: 1\t| Loss: 0.0007672553062438965\t\n",
      "epoch: 111\t| Loss: -0.00620499837026\t| Ctd: 0.8711871858891791\t| IBS: 0.04967393449007312\t| INBLL: 0.17591642589377748\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 112\t| Cnt: 1\t| Loss: 0.0003839544653892517\t\n",
      "epoch: 112\t| Loss: -0.0062862481474876405\t| Ctd: 0.8703342705019939\t| IBS: 0.04963088550405618\t| INBLL: 0.17596386766796196\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 113\t| Cnt: 1\t| Loss: 0.0006704827547073365\t\n",
      "epoch: 113\t| Loss: -0.006569801524281502\t| Ctd: 0.8712817274729252\t| IBS: 0.04975835580177876\t| INBLL: 0.1758127616397256\t\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 114\t| Cnt: 1\t| Loss: 0.00016931074857711793\t\n",
      "epoch: 114\t| Loss: -0.006078408136963844\t| Ctd: 0.8696312433399056\t| IBS: 0.04950436294425273\t| INBLL: 0.17593729159324442\t\n",
      "** ** * No improvement in model   ** ** * \n",
      "epoch: 115\t| Cnt: 1\t| Loss: -1.271963119506836e-05\t\n",
      "epoch: 115\t| Loss: -0.006445815440267324\t| Ctd: 0.8706042305183677\t| IBS: 0.04962949119109439\t| INBLL: 0.1757563312792838\t\n",
      "** ** * No improvement in model   ** ** * \n",
      "epoch: 116\t| Cnt: 1\t| Loss: -0.0007166908979415893\t\n",
      "epoch: 116\t| Loss: -0.006267477413639426\t| Ctd: 0.8705772970459\t| IBS: 0.04950699966819336\t| INBLL: 0.17585740091710206\t\n",
      "** ** * No improvement in model   ** ** * \n",
      "** ** * Stopping training       ** ** * \n",
      "Test results after: 116 epochs\t| Loss: -0.006569801524281502\t| Ctd: 0.8712817274729252\t| IBS: 0.04975835580177876\t| INBLL: 0.1758127616397256\t\n"
     ]
    }
   ],
   "source": [
    "max_wait = 3\n",
    "\n",
    "best_pre = np.inf\n",
    "wait = 0\n",
    "for e in range(1000):\n",
    "\n",
    "    # Train for an epoch\n",
    "    train(e)\n",
    "\n",
    "    # Validation \n",
    "    # Loss to check for early stopping\n",
    "    if file_config[\"val\"] is not None:\n",
    "        val_loss = validation(valload)\n",
    "    else:\n",
    "        val_loss = None\n",
    "    # Metrics\n",
    "    if file_config[\"test\"] is not None:\n",
    "        ctd, ibs, inbll = evaluation(valload)\n",
    "    else:\n",
    "        ctd, ibs, inbll = None, None, None\n",
    "\n",
    "    # Report\n",
    "    print(\"epoch: {}\\t| Loss: {}\\t| Ctd: {}\\t| IBS: {}\\t| INBLL: {}\\t\".format(e, val_loss, ctd, ibs, inbll ))\n",
    "    \n",
    "    if val_loss < best_pre:\n",
    "        # Save a trained model\n",
    "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
    "        wait = 0\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
    "        create_folder(global_params['output_dir'])\n",
    "        if global_params['save_model']:\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        best_pre = val_loss\n",
    "    else:\n",
    "        print(\"** ** * No improvement in model   ** ** * \")\n",
    "        wait += 1\n",
    "        if wait >= max_wait:\n",
    "            print(\"** ** * Stopping training       ** ** * \")\n",
    "            model_dict = torch.load(output_model_file)\n",
    "            model.load_state_dict(model_dict)\n",
    "            \n",
    "            tst_loss = validation(testload)\n",
    "            ctd, ibs, inbll = evaluation(testload)\n",
    "            print(\"Test results after: {} epochs\\t| Loss: {}\\t| Ctd: {}\\t| IBS: {}\\t| INBLL: {}\\t\".format(e, tst_loss, ctd, ibs, inbll ))\n",
    "\n",
    "            \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
